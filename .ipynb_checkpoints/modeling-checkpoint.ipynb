{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, brier_score_loss, log_loss\n",
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, make_scorer, roc_curve, auc\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split, learning_curve, ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.utils import resample\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a random_grid for going over hyperparameters\n",
    "def opt_random_forest(X_train,y_train,X_test,y_test,labels):\n",
    "    \n",
    "    \n",
    "    scorer = make_scorer(precision_score)\n",
    "\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 10, stop = 500, num = 3)]\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 4]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap};\n",
    "\n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestClassifier(random_state = 42);\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rf_random = GridSearchCV(estimator = rf, \n",
    "                             param_grid = random_grid, \n",
    "                             scoring=scorer,\n",
    "                             cv = 3, \n",
    "                             verbose=2, \n",
    "                             n_jobs = -1);\n",
    "    # Fit the random search model\n",
    "\n",
    "    rf_random.fit(X_train, y_train);\n",
    "\n",
    "    base_model = RandomForestClassifier(n_estimators = 10, random_state = 42);\n",
    "    pl = 1\n",
    "    which_model = \"Base Random Forest\"\n",
    "    base_model.fit(X_train, y_train);\n",
    "    base_accuracy = evaluate(base_model, X_test, y_test,labels,pl,which_model)\n",
    "    \n",
    "\n",
    "    print(rf_random.best_params_)\n",
    "    best_random = rf_random.best_estimator_\n",
    "    which_model = \"Optimized Random Forest\"\n",
    "    random_accuracy = evaluate(best_random, X_test, y_test,labels, pl, which_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def opt_log_reg(X_train,y_train,X_test,y_test,labels):\n",
    "    # Build a random_grid for going over hyperparameters\n",
    "    scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "\n",
    "    # Create regularization penalty space\n",
    "    penalty = ['l1']\n",
    "\n",
    "    # Create regularization hyperparameter distribution using uniform distribution\n",
    "    C = np.array(np.linspace(0.5,1,10))\n",
    "\n",
    "    # Create hyperparameter options\n",
    "    random_grid = dict(C=C, penalty=penalty);\n",
    "\n",
    "\n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    lg = LogisticRegression(solver ='liblinear', random_state=42);\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    lg_random = GridSearchCV(estimator = lg, \n",
    "                                   param_grid = random_grid,\n",
    "                                   scoring = scorer,\n",
    "                                   cv = 3, \n",
    "                                   verbose=2, \n",
    "                                   n_jobs = -1);\n",
    "\n",
    "    # Fit the random search model\n",
    "    lg_random.fit(X_train, y_train);\n",
    "\n",
    "    lg_random.best_params_\n",
    "\n",
    "    base_model = LogisticRegression(random_state = 42, solver ='liblinear');\n",
    "    pl = 1\n",
    "    which_model = \"Base Logistic Regression\"\n",
    "    base_model.fit(X_train, y_train);\n",
    "    base_accuracy = evaluate(base_model, X_test, y_test,labels,pl, which_model)\n",
    "\n",
    "    print(lg_random.best_params_)\n",
    "    best_random = lg_random.best_estimator_\n",
    "    which_model = \"Optimized Logistic Regression\"\n",
    "    random_accuracy = evaluate(best_random, X_test, y_test,labels, pl, which_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_xgboost(X_train,y_train,X_test,y_test,labels):\n",
    "    # Create XGB Classifier object\n",
    "\n",
    "    scorer = make_scorer(precision_score);\n",
    "    xgb_clf = xgb.XGBClassifier(tree_method = \"auto\", predictor = \"cpu_predictor\", verbosity = 0,\n",
    "                               eval_metric = [\"error\",\"map\", \"auc\"], objective = \"binary:logistic\", seed =42);\n",
    "    # Create parameter grid\n",
    "    parameters = {\"learning_rate\": [0.1, 0.01, 0.001],\n",
    "                   \"gamma\" : [0.01, 0.1, 0.3, 0.5, 1, 1.5, 2],\n",
    "                   \"max_depth\": [2, 4, 7, 10],\n",
    "                   \"colsample_bytree\": [0.3, 0.6, 0.8, 1.0],\n",
    "                   \"subsample\": [0.2, 0.4, 0.5, 0.6, 0.7],\n",
    "                   \"reg_alpha\": [0, 0.5, 1],\n",
    "                   \"reg_lambda\": [1, 1.5, 2, 3, 4.5],\n",
    "                   \"min_child_weight\": [1, 3, 5, 7],\n",
    "                   \"n_estimators\": [100, 250, 500, 1000]};\n",
    "\n",
    "    # Create RandomizedSearchCV Object\n",
    "    xgb_rscv = RandomizedSearchCV(xgb_clf, \n",
    "                                  param_distributions = parameters, \n",
    "                                  scoring=scorer,\n",
    "                                  cv = 3, \n",
    "                                  verbose = 1, \n",
    "                                  random_state = 42);\n",
    "\n",
    "    # Fit the model\n",
    "    pl = 1\n",
    "\n",
    "    model_xgboost = xgb_rscv.fit(X_train, y_train);\n",
    "\n",
    "    print(model_xgboost.best_params_)\n",
    "\n",
    "    best_random_xgb = model_xgboost.best_estimator_\n",
    "    which_model = \"XGBoost\"\n",
    "    accuracy = evaluate(best_random_xgb, X_test, y_test,labels, pl, which_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels(binary,labels):\n",
    "    labelled = [labels[x] for x in binary]\n",
    "    return labelled\n",
    "\n",
    "def evaluate(model, X_test, y_test, labels,pl, which_model):\n",
    "    try:\n",
    "        print(\"Model coefficients:\",model.coef_)\n",
    "    except:\n",
    "        print(\"Feature importances:\", model.feature_importances_)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[::,1]\n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    y_axis_labels = ['Test','Predictions','Probabilities']\n",
    "    sns.heatmap([y_test,predictions,y_prob], square=True, \n",
    "                                             cmap='plasma',\n",
    "                                             cbar=False, \n",
    "                                             yticklabels=y_axis_labels)\n",
    "    test_label = assign_labels(y_test,labels)\n",
    "    pred_label = assign_labels(predictions,labels)\n",
    "    confusion_mat = confusion_matrix(test_label,pred_label)\n",
    "\n",
    "    print(classification_report(y_test, predictions, target_names=labels))\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test,y_prob,pos_label=pl)\n",
    "    AUC = auc(fpr, tpr)\n",
    "    print('Confusion matrix: ')\n",
    "    print('Predicted')\n",
    "    print(\"     \",labels)\n",
    "    print(\"Test \",labels[0],confusion_mat[0])\n",
    "    print(\"     \",labels[1],confusion_mat[1])\n",
    "    \n",
    "    result_table = {'fpr':fpr, 'tpr':tpr, 'auc':AUC}\n",
    "\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    \n",
    "    plt.plot(result_table['fpr'], \n",
    "             result_table['tpr'], \n",
    "             label=\"AUC={:.3f}\".format(result_table['auc']))\n",
    "    \n",
    "    plt.plot([0,1], [0,1], color='black', linestyle='--')\n",
    "\n",
    "    plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
    "\n",
    "    plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "\n",
    "    plt.title('ROC Curve Analysis for '+which_model, fontweight='bold', fontsize=15)\n",
    "    plt.legend(prop={'size':13}, loc='lower right')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    From: https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    scorer = make_scorer(f1_score)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scorer)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abundance_comparison(df, IDs,norm):\n",
    "    \n",
    "    df = df[IDs+['Group']]\n",
    "    df_long = pd.melt(df, \"Group\", var_name=\"Taxon\", value_name=\"abundance\")\n",
    "    df_long = df_long[~(df_long == 0).any(axis=1)]\n",
    "    if norm=='clr':\n",
    "        df_long['abundance'] = df_long['abundance'].apply(lambda x: math.exp(x))\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    sns.boxplot(x=\"Taxon\", y=\"abundance\",\n",
    "            hue=\"Group\",\n",
    "            data=df_long)\n",
    "    #plt.ylim(0, 25000)\n",
    "\n",
    "    for i in IDs:\n",
    "        grouped = df.groupby(\"Group\")[i]\n",
    "        value_dict ={}\n",
    "        dists = []\n",
    "        for key, item in grouped:\n",
    "            dists.append(grouped.get_group(key).values)\n",
    "        print(\"For OSU ID {} , t-test statistic: \".format(i),\n",
    "              stats.ttest_ind(dists[0], dists[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

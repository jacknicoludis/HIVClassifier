{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skbio.stats.composition import clr\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input variables for CSS normalization\n",
    "instability_metric = 0.05 #difference between nearest difference in qL variance\n",
    "N = 100000 #count multiplier\n",
    "\n",
    "def clr_normalization(osu_df):\n",
    "    '''Atchinson centered-log ratio of the composition data'''\n",
    "    \n",
    "    \n",
    "    #pivot the dataframe so samples are rows and osu_ids (features) are columns\n",
    "    inds = 'sample_id'\n",
    "    vals = 'osu_count'\n",
    "    cols = 'osu_id'\n",
    "    osu_df = pivot_osu_df(osu_df, inds,vals,cols)\n",
    "\n",
    "    #Replace zeros with a pseudocount of 0.5\n",
    "    osu_df = osu_df.replace(0,0.5)\n",
    "\n",
    "    cols = osu_df.columns\n",
    "    inds = osu_df.index\n",
    "    \n",
    "    #Do the center-log ratio\n",
    "    osu_lt = clr(osu_df)\n",
    "    osu_df = pd.DataFrame(data=osu_lt, columns=cols, index=inds)\n",
    "    \n",
    "    return osu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tss_normalization(osu_df):\n",
    "    '''Total sum scaling of dataset'''\n",
    "    \n",
    "    #normalize columns by the sum of the column\n",
    "    osu_df = osu_df.assign(norm_count=osu_df.groupby('sample_id', group_keys=False)\n",
    "                           .apply(lambda x: x.osu_count/x.osu_count.sum())) \n",
    "    \n",
    "    #Pivot dataframe so sampeles are rows and osu_ids (features) are columns\n",
    "    inds = 'sample_id'\n",
    "    vals = 'norm_count'\n",
    "    cols = 'osu_id'\n",
    "    osu_df = pivot_osu_df(osu_df, inds,vals,cols)\n",
    "    \n",
    "    return osu_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def css_normalization(osu_df):\n",
    "    '''Cumulative sum-scaling'''\n",
    "    \n",
    "    # Pivot DataFrame\n",
    "    inds = 'osu_id'\n",
    "    vals = 'osu_count'\n",
    "    cols = 'sample_id'\n",
    "    osu_df = pivot_osu_df(osu_df, inds,vals,cols)\n",
    "\n",
    "    #Remove Group from DataFrame\n",
    "    cols = osu_df.columns.tolist()\n",
    "    #osu_df = osu_df.drop(['Group'],axis=1)\n",
    "\n",
    "    osu_df = osu_df.fillna(0)\n",
    "    #osu_df.columns = [col[1] for col in osu_df.columns]\n",
    "\n",
    "    #Dictionary of total counds in columns\n",
    "    n_counts = {}\n",
    "    for col in osu_df.columns.tolist():\n",
    "        n_counts[col] = osu_df[col].astype(bool).sum(axis=0)\n",
    "\n",
    "    #For quantile from 0 to 1, count the number of taxons with at least \n",
    "    #this many read counts\n",
    "    cs = pd.DataFrame()\n",
    "\n",
    "    for col in osu_df.columns.tolist():\n",
    "        \n",
    "        sorted_counts = osu_df[col].sort_values()\n",
    "        sorted_counts = sorted_counts[sorted_counts >0]\n",
    "        for l in np.linspace(0,1,num=100):\n",
    "            ln = int(l*(n_counts[col]-1))\n",
    "            temp_dict = {'sample_id':col,'l':l,'ql':sorted_counts.iloc[ln]}\n",
    "            cs = pd.concat([cs,pd.DataFrame.from_records([temp_dict], index='sample_id')])\n",
    "            \n",
    "    inds = 'l'\n",
    "    vals = 'ql'\n",
    "    cols = 'sample_id'\n",
    "\n",
    "    #pivot table\n",
    "    cs = pd.pivot_table(cs, \n",
    "                        index=[inds],\n",
    "                        values=[vals],\n",
    "                        columns=[cols])\n",
    "    cs.columns = [col[1] for col in cs.columns]\n",
    "\n",
    "    #The median qL for all samples at each quantile\n",
    "    dl =cs.median(axis=1).tolist()\n",
    "\n",
    "    #The median difference between the median qL and and each samples quantiles\n",
    "    dl_median = cs.sub(dl, axis='index').abs().median(axis=1)\n",
    "    dl_median.plot()\n",
    "\n",
    "    dl_med_list = dl_median.tolist()\n",
    "\n",
    "    #Calculate the cut-off value based on difference in median qL variance from one\n",
    "    #quantile to the next being less than the <instability_metric> * qL value\n",
    "    for i in range(len(dl_med_list[:-1])):\n",
    "\n",
    "        dl_i = dl_med_list[len(dl_med_list)-i-1]\n",
    "        dl_i1 = dl_med_list[len(dl_med_list)-i-2]\n",
    "\n",
    "        dl_diff = abs(dl_i-dl_i1)\n",
    "        if dl_diff < dl_i*instability_metric:\n",
    "            l_hat_index = i\n",
    "            break\n",
    "        else:\n",
    "            l_hat_index = len(dl_med_list)\n",
    "\n",
    "    # The quantile that indicates the normalization constant\n",
    "    l_hat = dl_median.index.tolist()[len(dl_med_list)-l_hat_index]\n",
    "\n",
    "    #Normalization constant \n",
    "    sjs = cs[cs.index > l_hat].sum(axis=0)\n",
    "\n",
    "    #Divide counts by the normalization constants\n",
    "    osu_df = osu_df.div(sjs,axis=1)\n",
    "    osu_df = osu_df*N #multiple by the count multiplier\n",
    "    osu_df = osu_df.T #take the Transform so DF is in the same format at others\n",
    "    \n",
    "    return osu_df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "   def pivot_osu_df(osu_df, inds,vals,cols):\n",
    "    '''Function to Pivot data frame, drop the multilevel column and fill NaNs with zeros.'''\n",
    "    osu_df = pd.pivot_table(osu_df, \n",
    "                                index=[inds],\n",
    "                                values=[vals],\n",
    "                                columns=[cols],\n",
    "                                aggfunc=np.sum)\n",
    "    osu_df = osu_df.sort_index(axis=0)\n",
    "    osu_df.columns = osu_df.columns.droplevel()\n",
    "    osu_df = osu_df.fillna(0)\n",
    "    \n",
    "    return osu_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_osus(files,norm_type):\n",
    "    '''\n",
    "    Joins osu_abundance files and normalizes in one of three ways:\n",
    "    \n",
    "    Parameters:\n",
    "    files: \n",
    "    list of files that are output from HiMAP\n",
    "    \n",
    "    Norm_type:\n",
    "    Type of normalization from the following:\n",
    "    tss - total sum scaling: divide each sample by total sum of reads\n",
    "    clr - Atchinson's centered log-ratio\n",
    "    css - Cumulative sum scaling\n",
    "    \n",
    "    Returns a dataframe containing the samples as rows and OSUs as columns\n",
    "    '''\n",
    "    osu_df = pd.DataFrame()\n",
    "\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file,sep='\\t')\n",
    "        osu_df = pd.concat([osu_df, df])\n",
    "    osu_df = osu_df.reset_index()\n",
    "    \n",
    "    if norm_type == 'tss':\n",
    "        \n",
    "        osu_df = tss_normalization(osu_df)\n",
    "        \n",
    "    elif norm_type == 'clr':\n",
    "        \n",
    "        osu_df = clr_normalization(osu_df)\n",
    "        \n",
    "    elif norm_type == 'css':\n",
    "        \n",
    "        osu_df = css_normalization(osu_df)\n",
    "        \n",
    "    return osu_df\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_taxonomy(files):\n",
    "    \n",
    "    tax_df = pd.DataFrame()\n",
    "    \n",
    "    for file in files:\n",
    "        df = pd.read_csv(file,sep='\\t')\n",
    "        tax_df = pd.concat([tax_df, df])\n",
    "    \n",
    "    tax_df = tax_df.drop_duplicates(keep='first')\n",
    "    \n",
    "    try:\n",
    "        tax_df = tax_df.drop(['pctsim'], axis=1)\n",
    "    except:\n",
    "        None\n",
    "    \n",
    "    return tax_df\n",
    "\n",
    "def get_labels(meta_file):\n",
    "    labels = 'labels'\n",
    "    meta_df = pd.read_csv(meta_file,sep=',',index_col='run_accession')\n",
    "\n",
    "    return meta_df\n",
    "        \n",
    "def normalize_counts(osu_df):\n",
    "    '''Normalizes counts of an OSU DataFrame grouping by <sample_id> and summing <osu_count> '''\n",
    "    \n",
    "    osu_df = osu_df.assign(norm_count=osu_df.groupby('sample_id', group_keys=False)\n",
    "                           .apply(lambda x: x.osu_count/x.osu_count.sum())) \n",
    "    return osu_df\n",
    "\n",
    "\n",
    "def join_osu_with_labels(osu_df,meta_df):\n",
    "    \n",
    "    group = meta_df.Group\n",
    "    #sample = meta_df.Sample\n",
    "    osu_df = pd.concat([osu_df, group], axis=1, sort=True)\n",
    "    osu_df = osu_df.fillna(0)\n",
    "    \n",
    "    \n",
    "    return osu_df\n",
    "\n",
    "def select_groups(df,groups):\n",
    "    groups_df=pd.DataFrame()\n",
    "    for g in groups:\n",
    "        group_g = df['Group'] == g\n",
    "        new_df = df[group_g]\n",
    "        groups_df = pd.concat([groups_df, new_df], sort=True)\n",
    "        \n",
    "    return groups_df\n",
    "\n",
    "def get_sample_weights(y):\n",
    "    one_sum = sum(y)\n",
    "    zero_sum = len(y)-one_sum\n",
    "    if one_sum> zero_sum:\n",
    "        scale = one_sum/zero_sum\n",
    "        sw = np.array([1 if i == 1 else scale for i in y])\n",
    "    elif zero_sum > one_sum:\n",
    "        scale = zero_sum/one_sum\n",
    "        sw = np.array([1 if i == 0 else scale for i in y])\n",
    "    else:\n",
    "        sw = np.array([1 for i in y])\n",
    "        \n",
    "    return sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Reductions\n",
    "\n",
    "### Implementation of feature reduction methods:\n",
    "\n",
    "`make_df_up_level` - converts OSU_ID to 'genus', 'family'... level\n",
    "\n",
    "### These methods convert dataframe of data to dataset for making train/test split:\n",
    "\n",
    "`SVD_truncate` - reduces dimensionality of features by SVD truncate\n",
    "\n",
    "`make_dataset_osu_diff` - Rank features by difference in mean abundance between labels\n",
    "\n",
    "`make_dataset` - Converts dataframe to dataset for making train/test split\n",
    "\n",
    "`make_dataset_zscore` - Rank features by Z-score between labels\n",
    "\n",
    "`feature_from_correlation` - Rank features by their correlation with the label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_up_level(osu_df,tax_df,level,norm_type):\n",
    "    '''\n",
    "    Changes taxonomic level of dataframe to a user-specified level\n",
    "    Input: osu_df - dataframe of abundacnes\n",
    "           tax_df - dataframe of taxonomic information\n",
    "           level - level to which taxonomic information should be moved to.\n",
    "                   Can be None, genus, family, order, class, phylum\n",
    "           norm_type - type of normalization used\n",
    "    Returns: dataframe with new taxonomy information\n",
    "    '''\n",
    "    #Make dictionary of the OSU ids the the taxonomy at a specific level\n",
    "    up_dict = dict(zip(tax_df.osu_id, tax_df[level]))\n",
    "    \n",
    "    #Replace brackets with underscores because of issues\n",
    "    for key in up_dict:\n",
    "        val = str(up_dict[key])\n",
    "        val = val.replace(']','_')\n",
    "        val = val.replace('[','_')\n",
    "        up_dict[key]=val\n",
    "    \n",
    "    #Replace osu ids with the taxonomy level\n",
    "    df = osu_df.rename(columns=up_dict)\n",
    "    cols = df.columns.tolist()\n",
    "    \n",
    "    #Drop osu_ids without taxonomy information\n",
    "    cols = [x for x in cols if \"_\" not in str(x)]\n",
    "    df = df.drop(cols, axis=1)\n",
    "    \n",
    "    #Convert abundance data from log to normal if the clr normalization is used\n",
    "    if norm_type == 'clr':\n",
    "        df = df.applymap(np.exp)\n",
    "    \n",
    "    #Sum data with the same taxonomy\n",
    "    df = df.groupby(by=df.columns, axis=1).sum()\n",
    "    \n",
    "    #Re-log the data if clr normalization was used\n",
    "    if norm_type == 'clr':\n",
    "        df = df.applymap(np.log)\n",
    "        \n",
    "    #Reconcatenate group information onto dataframe\n",
    "    df = pd.concat([df,osu_df.Group],axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVD_truncate(df,ncomp,cutoff):\n",
    "    '''\n",
    "    Uses SVD-truncate to reduce features\n",
    "    Inputs: df - dataframe containing the composition data\n",
    "            ncomp - number of components\n",
    "            cutoff - the abundance level cutoff\n",
    "    Return: X - array of sample-feature data\n",
    "            Y - binary labels for data\n",
    "            labels - text labels for data\n",
    "    '''\n",
    "    #Convert cutoff to log scale if clr normalization used\n",
    "    if norm_type == 'clr':\n",
    "        cutoff = np.e**cutoff\n",
    "        \n",
    "    #Identify unique labels\n",
    "    labels = df.Group.unique()\n",
    "\n",
    "    #Convert group labels to binary labels and save as Y\n",
    "    d_cat ={}\n",
    "    for i,cat in enumerate(labels):\n",
    "        d_cat[cat]=i\n",
    "    Y = [d_cat[x] for x in df['Group'].tolist()]\n",
    "    \n",
    "    #Drop group data, transpose, remove data where all counts are under the cutoff and re-transpose\n",
    "    df = df.drop(['Group'], axis=1)\n",
    "    dfT = df.T\n",
    "    dfT = dfT[~(dfT[dfT.columns] < cutoff).all(axis=1)]\n",
    "    df = dfT.T\n",
    "    \n",
    "    #Make dataframe \n",
    "    X = np.array(df)\n",
    "    \n",
    "    #Use Truncated SVD and plot\n",
    "    svd = TruncatedSVD(n_components=ncomp, n_iter=10, random_state=42)\n",
    "    svd_result = svd.fit(X) \n",
    "    X = svd.fit_transform(X)\n",
    "    print(\"Explained variance ratios:\")\n",
    "    print(svd_result.explained_variance_ratio_)\n",
    "    print('Sum of explained variance ratios:',np.sum(svd_result.explained_variance_ratio_))\n",
    "    plt.figure(figsize=((15,4)))\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.bar(x=np.linspace(0,ncomp,ncomp),height = svd_result.explained_variance_ratio_)\n",
    "    \n",
    "    return X, Y, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_osu_diff(df, ncomp, cutoff):\n",
    "    '''\n",
    "    Returns top <ncomp> features based on difference in mean abundance between labels\n",
    "    Inputs: df - dataframe containing the composition data\n",
    "            ncomp - number of components\n",
    "            cutoff - the abundance level cutoff\n",
    "    Return: X - array of sample-feature data\n",
    "            Y - binary labels for data\n",
    "            labels - text labels for data\n",
    "    '''\n",
    "    #Change cutoff if norm_type is clr\n",
    "    if norm_type == 'clr':\n",
    "        cutoff = np.e**cutoff\n",
    "        \n",
    "    #Get labels and convert labels to binary\n",
    "    labels = df.Group.unique()\n",
    "    d_cat ={}\n",
    "    for i,cat in enumerate(labels):\n",
    "        d_cat[cat]=i\n",
    "    Y = [d_cat[x] for x in df['Group'].tolist()]\n",
    "    \n",
    "    #Set Group as index and transpose\n",
    "    df = df.set_index(['Group'])\n",
    "    dfT = df.T\n",
    "    \n",
    "    #Remove features that don't pass cutoff filter\n",
    "    dfT = dfT[~(dfT[dfT.columns] < cutoff).all(axis=1)]\n",
    "    \n",
    "    #Group feature data by label using mean\n",
    "    dfT = dfT.groupby(by=dfT.columns, axis=1).mean()\n",
    "\n",
    "    #Calculate difference in mean\n",
    "    dfT['diff']=abs(dfT[dfT.columns[0]]-dfT[dfT.columns[1]])\n",
    "    \n",
    "    #Sort by difference in mean\n",
    "    dfT = dfT.sort_values(by=['diff'],ascending=False)\n",
    "    \n",
    "    #Select top <ncomp> features\n",
    "    inds = dfT.index.tolist()[0:ncomp]\n",
    "    df = df[inds]\n",
    "    X = np.array(df)\n",
    "\n",
    "    return X,Y,labels,inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df, cutoff):\n",
    "    '''\n",
    "    Returns dataset in format for X, Y, labels\n",
    "    Inputs: df - dataframe containing the composition data\n",
    "            ncomp - number of components\n",
    "            cutoff - the abundance level cutoff\n",
    "    Return: X - array of sample-feature data\n",
    "            Y - binary labels for data\n",
    "            labels - text labels for data\n",
    "    '''\n",
    "    \n",
    "    #Change cutoff if norm_type is clr\n",
    "    if norm_type == 'clr':\n",
    "        cutoff = np.e**cutoff\n",
    "    \n",
    "    #Get labels and convert labels to binary\n",
    "    labels = df.Group.unique()\n",
    "    d_cat ={}\n",
    "    for i,cat in enumerate(labels):\n",
    "        d_cat[cat]=i\n",
    "    Y = [d_cat[x] for x in df['Group'].tolist()]\n",
    "    \n",
    "    #Set Group as index and transpose\n",
    "    df = df.set_index(['Group'])\n",
    "    dfT = df.T\n",
    "    \n",
    "    #Remove features that don't pass cutoff filter\n",
    "    dfT = dfT[~(dfT[dfT.columns] < cutoff).all(axis=1)]\n",
    "    inds = dfT.index.tolist()\n",
    "    df = df[inds]\n",
    "    X = np.array(df)\n",
    "\n",
    "    return X,Y,labels,inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_zscore(df, ncomp, cutoff, plot_cutoff,norm_type):\n",
    "    '''\n",
    "    Returns top <ncomp> features based on zscore\n",
    "    Inputs: df - dataframe containing the composition data\n",
    "            ncomp - number of components\n",
    "            cutoff - the abundance level cutoff\n",
    "    Return: X - array of sample-feature data\n",
    "            Y - binary labels for data\n",
    "            labels - text labels for data\n",
    "    '''\n",
    "    #convert cutoff to log scale if normalization is clr\n",
    "    if norm_type == 'clr':\n",
    "        cutoff = np.e**cutoff\n",
    "        \n",
    "    #rename DF\n",
    "    dfn = df\n",
    "    \n",
    "    #get group labels and convert to binary\n",
    "    labels = df.Group.unique()\n",
    "    d_cat ={}\n",
    "    for i,cat in enumerate(labels):\n",
    "        d_cat[cat]=i\n",
    "    Y = [d_cat[x] for x in df['Group'].tolist()]\n",
    "    \n",
    "    #Set index to Group\n",
    "    dfn = dfn.set_index(['Group'])\n",
    "    \n",
    "    #Take transpose\n",
    "    dfT = dfn.T\n",
    "    \n",
    "    #Remove columns where all values below cutoff\n",
    "    dfT = dfT[~(dfT[dfT.columns] < cutoff).all(axis=1)]\n",
    "    \n",
    "    #Take mean of each feature for the two groups\n",
    "    dfTmean = dfT.groupby(by=dfT.columns, axis=1).mean()\n",
    "    \n",
    "    #Take std of each feature for the two groups\n",
    "    dfTstd = dfT.groupby(by=dfT.columns, axis=1).std()\n",
    "    \n",
    "    #Rename std columns and concatenate the two\n",
    "    dfTstd.columns = [x+\"_std\" for x in dfTstd.columns.tolist()]\n",
    "    dfT = pd.concat([dfTmean,dfTstd],axis=1)\n",
    "\n",
    "    #Make z-score column and sort\n",
    "    dfT['zscore']=abs((dfT[dfT.columns[0]]-dfT[dfT.columns[1]])/((dfT[dfT.columns[2]]+dfT[dfT.columns[3]])/2))\n",
    "    dfT = dfT.sort_values(by=['zscore'],ascending=False)\n",
    "    \n",
    "    #Plot\n",
    "    plt.figure(figsize=((15,4)))\n",
    "    dfT[dfT['zscore']>plot_cutoff].plot.bar(y = 'zscore')\n",
    "    \n",
    "    #Take top components\n",
    "    inds = dfT.index.tolist()[0:ncomp]\n",
    "    df = df[inds]\n",
    "    X = np.array(df)\n",
    "\n",
    "    return X,Y,labels,inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_from_correlation(df,ncomp,cutoff,norm_type):\n",
    "    '''\n",
    "    Returns top <ncomp> features based on correlation with labels\n",
    "    Inputs: df - dataframe containing the composition data\n",
    "            ncomp - number of components\n",
    "            cutoff - the abundance level cutoff\n",
    "    Return: X - array of sample-feature data\n",
    "            Y - binary labels for data\n",
    "            labels - text labels for data\n",
    "    '''\n",
    "    #convert cutoff to log scale if normalization is clr\n",
    "    if norm_type == 'clr':\n",
    "        cutoff = np.e**cutoff\n",
    "    \n",
    "    #rename DF\n",
    "    dfn = df\n",
    "    \n",
    "    #make a list of the Group labels\n",
    "    group = dfn.Group\n",
    "    \n",
    "    #Drop group labels\n",
    "    dfn = dfn.drop(['Group'],axis=1)\n",
    "    \n",
    "    #Transpose, remove columns where all values are below cutoff, retranspose\n",
    "    dfT = dfn.T\n",
    "    dfT = dfT[~(dfT[dfT.columns] < cutoff).all(axis=1)]\n",
    "    dfn = dfT.T\n",
    "    \n",
    "    #Add Group labels back\n",
    "    dfn = pd.concat([dfn,group],axis=1)\n",
    "\n",
    "    #find unique group labels and replace with a binary\n",
    "    labels = df.Group.unique()\n",
    "    for i,cat in enumerate(labels):\n",
    "        dfn = dfn.replace(cat,i)\n",
    "\n",
    "    #calculate correlation\n",
    "    cor = dfn.corr()\n",
    "    \n",
    "    #Correlation with output variable\n",
    "    cor_target = abs(cor[\"Group\"])\n",
    "    \n",
    "    #Selecting highly correlated features\n",
    "    relevant_features = cor_target.sort_values(ascending=False).head(ncomp+1)\n",
    "    \n",
    "    #select top correlating features and drop group\n",
    "    dfn = dfn[relevant_features.index]\n",
    "    Y = dfn['Group'].tolist()\n",
    "    X = dfn.drop(['Group'],axis=1)\n",
    "    \n",
    "    return X,Y,labels,relevant_features.index.tolist()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

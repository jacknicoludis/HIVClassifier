{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run_entire_model\n",
    "\n",
    "Wrapper for other functions to run the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "#Load other functions\n",
    "%run feature_reductions.ipynb\n",
    "%run join_and_normalize.ipynb\n",
    "%run modeling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label name is 'Group' for the HIV dataset\n",
    "\n",
    "def run_entire_model(osu_files, \n",
    "                     tax_files, \n",
    "                     meta_file, \n",
    "                     norm_type,\n",
    "                     ncomp,\n",
    "                     levelup, \n",
    "                     groups,\n",
    "                     feat_reduction, \n",
    "                     test_frac, \n",
    "                     model,\n",
    "                     plot_pca,\n",
    "                     plot_lc,\n",
    "                     plot_ab_comp,\n",
    "                     cutoff,\n",
    "                     scorer):\n",
    "    '''\n",
    "    Wrapper script to run the HIV Classifier!\n",
    "    Inputs: `osu_files` - list of locations of osu_abundances.txt files\n",
    "            `tax_files` - list of locations of osu_taxonomy.txt files\n",
    "            `meta_file` - str of location of meta data file\n",
    "            `norm_type` - type of normalization: 'clr', 'css', 'tss'\n",
    "            `ncomp` - integer number of components\n",
    "            `levelup` - taxonomy level to group data: None, 'genus', 'order','class', 'family', 'phylum'\n",
    "            `groups` - List of two groups: e.g. ['NEG','RHI']\n",
    "            `feat_reduction` - Feat reduction method: None, 'svd', 'zscore', 'corr', 'diff'\n",
    "            `test_frac` - Test fraction (e.g. 0.25)\n",
    "            `model` - type of model: 'lg', 'rf', 'xgb', 'all'\n",
    "            `plot_pca` - Bool of whether to plot PCA\n",
    "            `plot_lc` - Bool to plot learning curve \n",
    "            `plot_ab_comp` - Bool to plot abundance comparison\n",
    "            `cutoff` - cutoff level (e.g. 0.001)\n",
    "            `scorer` - type of score: precision_score, recall_score, f1_score, fbeta_score, roc_auc_score\n",
    "    Returns: Report of performance\n",
    "    '''\n",
    "    \n",
    "    #Make dataframe of OSU data and normalize according to <norm_type>\n",
    "    osu_df = join_osus(osu_files,norm_type)\n",
    "    \n",
    "    #Get label meta data\n",
    "    meta_df = get_labels(meta_file)\n",
    "    \n",
    "    #Join OSU data with meta data and load taxonomy data\n",
    "    osu_df = join_osu_with_labels(osu_df,meta_df)\n",
    "    tax_df = join_taxonomy(tax_files)\n",
    "    \n",
    "    #Select only the comparison <groups>\n",
    "    pair_df = select_groups(osu_df,groups)\n",
    "\n",
    "    #If selected, level up the taxonomy information\n",
    "    if levelup != None:\n",
    "        pair_df = make_df_up_level(pair_df,tax_df,levelup,norm_type)\n",
    "    \n",
    "    #Use feature reduction strategy\n",
    "    feats = None\n",
    "    if feat_reduction =='svd':\n",
    "        X,Y,labels = SVD_truncate(pair_df,ncomp,cutoff)\n",
    "    elif feat_reduction =='zscore':\n",
    "        plot_cutoff = 0.4\n",
    "        X,Y,labels,feats = make_dataset_zscore(pair_df, ncomp,cutoff,plot_cutoff,norm_type)\n",
    "    elif feat_reduction =='corr':\n",
    "        X,Y,labels,feats = feature_from_correlation(pair_df,ncomp,cutoff,norm_type)\n",
    "    elif feat_reduction =='diff':\n",
    "        X,Y,labels,feats = make_dataset_osu_diff(pair_df, ncomp,cutoff)\n",
    "    elif feat_reduction == None:\n",
    "        X,Y,labels,feats = make_dataset(pair_df,cutoff)\n",
    "    \n",
    "    print(\"Comparing the following groups:\",labels)\n",
    "\n",
    "    #Print the top features if feature reduction is chosen\n",
    "    if feats !=None:\n",
    "        if len(feats)<50:\n",
    "            print(\"Top features:\",feats)\n",
    "        \n",
    "    #Plot PCA data \n",
    "    if plot_pca == True:\n",
    "        if feat_reduction == 'svd':\n",
    "            #PCA does not need to be done if SVD was already done\n",
    "            X1 = X[:, 0]\n",
    "            X2 = X[:, 1]\n",
    "        else: \n",
    "            X_PCA = PCA(n_components=2, random_state=42).fit_transform(np.array(X))\n",
    "            X1 = X_PCA[:, 0]\n",
    "            X2 = X_PCA[:, 1]\n",
    "        c=np.array(Y)\n",
    "        colors = np.where(c == 0, 'r', 'k')\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.scatter(X1, X2, c=colors)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    #Split into training and test\n",
    "    seed = 30\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y,\n",
    "                                                    stratify=Y,\n",
    "                                                    test_size=test_frac,\n",
    "                                                    random_state=seed)\n",
    "    \n",
    "    \n",
    "    #Optimize classifier(s)\n",
    "    if model == 'lg':\n",
    "        print('-'*50)\n",
    "        print('Logistic Regression')\n",
    "        result_table = opt_log_reg(X_train,y_train,X_test,y_test,labels,scorer)\n",
    "    elif model == 'rf':\n",
    "        print('-'*50)\n",
    "        print('Random Forest')\n",
    "        result_table = opt_random_forest(X_train,y_train,X_test,y_test,labels,scorer)\n",
    "    elif model == 'xg':\n",
    "        print('-'*50)\n",
    "        print('XG Boost')\n",
    "        opt_xgboost(X_train,y_train,X_test,y_test,labels,scorer)\n",
    "    elif model == 'all':\n",
    "        print('-'*50)\n",
    "        print('Logistic Regression')\n",
    "        opt_log_reg(X_train,y_train,X_test,y_test,labels,scorer)\n",
    "        print('-'*50)\n",
    "        print('Random Forest')\n",
    "        opt_random_forest(X_train,y_train,X_test,y_test,labels,scorer)\n",
    "        print('-'*50)\n",
    "        print('XG Boost')\n",
    "        opt_xgboost(X_train,y_train,X_test,y_test,labels,scorer)\n",
    "       \n",
    "    #Plot comparison of features\n",
    "    if plot_ab_comp == True:\n",
    "        if feats == None:\n",
    "            print(\"Plotting abundance only applicable when zscore, diff, corr feature reduction used.\")\n",
    "        elif len(feats) > 20:\n",
    "            print(\"Too many features to plot effectively.\")\n",
    "        else:\n",
    "            IDs = feats\n",
    "            abundance_comparison(pair_df,IDs,norm_type)\n",
    "    \n",
    "    #Plot learning curve\n",
    "    if plot_lc == True:\n",
    "\n",
    "        title = \"Learning Curve for Logistic Regression\"\n",
    "        # Cross validation with 100 iterations to get smoother mean test and train\n",
    "        # score curves, each time with 20% data randomly selected as a validation set.\n",
    "        cv = ShuffleSplit(n_splits=100, test_size=test_frac, random_state=0)\n",
    "        \n",
    "        #get classweights\n",
    "        cw=get_class_weight(Y)\n",
    "        \n",
    "        #Use Logistic Regression \n",
    "        estimator = LogisticRegression(random_state = 42, solver ='liblinear',class_weight=cw);\n",
    "        plot_learning_curve(estimator, title, X, Y, ylim=(0.0, 1.01), cv=cv, n_jobs=10)\n",
    "\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
